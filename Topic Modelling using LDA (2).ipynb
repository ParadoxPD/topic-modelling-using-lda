{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\compilers\\python39\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: gensim in d:\\compilers\\python39\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: nltk in d:\\compilers\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pyLDAvis in d:\\compilers\\python39\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: tqdm in d:\\compilers\\python39\\lib\\site-packages (4.62.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\compilers\\python39\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\compilers\\python39\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: click in d:\\compilers\\python39\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in d:\\compilers\\python39\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\compilers\\python39\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "INFO: pip is looking at multiple versions of pyldavis to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E5BFA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E5B340>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E54880>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E54C40>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E661C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pandas/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E54AF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyldavis/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E54C10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyldavis/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E54640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyldavis/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E54760>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyldavis/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001ECA0E54D90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pyldavis/\n",
      "ERROR: Could not find a version that satisfies the requirement pandas>=2.0.0 (from pyldavis) (from versions: none)\n",
      "ERROR: No matching distribution found for pandas>=2.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.4.0 in d:\\compilers\\python39\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\compilers\\python39\\lib\\site-packages (from pandas==1.4.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\compilers\\python39\\lib\\site-packages (from pandas==1.4.0) (2020.5)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\compilers\\python39\\lib\\site-packages (from pandas==1.4.0) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\compilers\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas==1.4.0) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install numpy gensim nltk pyLDAvis tqdm\n",
    "!pip install pandas==1.4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "data = pd.read_csv('./abcnews-date-text.csv', on_bad_lines='skip')\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244184\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing the Raw data\n",
    "    \n",
    "    Steps :\n",
    "    1) Lemmatization\n",
    "    2) Stemming\n",
    "    3) Removing Stopwords\n",
    "    4) Removing words with low character counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return SnowballStemmer(language='english').stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mapping preprocessing to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1244184/1244184 [01:49<00:00, 11382.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# import multiprocessing\n",
    "# from multiprocessing import Pool\n",
    "# num_partitions = 5\n",
    "# num_cores = 1\n",
    "# a,b,c,d,e = np.array_split( documents['headline_text'],num_partitions)\n",
    "# pool = Pool(num_cores)\n",
    "tqdm.pandas()\n",
    "# df = pd.concat(pool.map(preprocess, [a,b,c,d,e]))\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "processed_docs = documents['headline_text'].progress_map(preprocess)\n",
    "# processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1244184/1244184 [00:08<00:00, 153071.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(tqdm(processed_docs))\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the data and keep 1000000 datapoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the data - **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1244184/1244184 [00:08<00:00, 152215.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(162, 1), (240, 1), (292, 1), (589, 1), (838, 1), (3571, 1), (3572, 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = []\n",
    "for i in tqdm(range(len(processed_docs))):\n",
    "    bow_corpus.append(dictionary.doc2bow(processed_docs[i]))\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 162 (\"govt\") appears 1 time.\n",
      "Word 240 (\"group\") appears 1 time.\n",
      "Word 292 (\"vote\") appears 1 time.\n",
      "Word 589 (\"local\") appears 1 time.\n",
      "Word 838 (\"want\") appears 1 time.\n",
      "Word 3571 (\"compulsori\") appears 1 time.\n",
      "Word 3572 (\"ratepay\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the data - **TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1244184/1244184 [00:00<00:00, 1248884.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5844216176085719),\n",
      " (1, 0.38716866963787633),\n",
      " (2, 0.5013820927104505),\n",
      " (3, 0.5071171375845095)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(tqdm(bow_corpus))\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1244184/1244184 [00:32<00:00, 38054.95it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(tqdm(bow_corpus), num_topics=10, id2word=dictionary, passes=2, workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.018*\"donald\" + 0.015*\"minist\" + 0.014*\"hospit\" + 0.010*\"drive\" + 0.009*\"hobart\" + 0.008*\"hunter\" + 0.008*\"river\" + 0.008*\"northern\" + 0.008*\"award\" + 0.008*\"video\"\n",
      "Topic: 1 \n",
      "Words: 0.022*\"crash\" + 0.016*\"die\" + 0.016*\"miss\" + 0.016*\"home\" + 0.014*\"perth\" + 0.013*\"bushfir\" + 0.012*\"death\" + 0.012*\"woman\" + 0.010*\"polic\" + 0.010*\"search\"\n",
      "Topic: 2 \n",
      "Words: 0.021*\"interview\" + 0.019*\"open\" + 0.018*\"market\" + 0.017*\"record\" + 0.014*\"final\" + 0.012*\"australian\" + 0.011*\"australia\" + 0.010*\"fall\" + 0.009*\"share\" + 0.009*\"guilti\"\n",
      "Topic: 3 \n",
      "Words: 0.021*\"elect\" + 0.020*\"chang\" + 0.014*\"hous\" + 0.013*\"busi\" + 0.013*\"council\" + 0.012*\"say\" + 0.012*\"labor\" + 0.011*\"farmer\" + 0.011*\"plan\" + 0.011*\"speak\"\n",
      "Topic: 4 \n",
      "Words: 0.031*\"queensland\" + 0.017*\"world\" + 0.016*\"win\" + 0.013*\"time\" + 0.011*\"royal\" + 0.010*\"beat\" + 0.010*\"australia\" + 0.009*\"commiss\" + 0.008*\"andrew\" + 0.008*\"port\"\n",
      "Topic: 5 \n",
      "Words: 0.031*\"court\" + 0.027*\"charg\" + 0.024*\"case\" + 0.024*\"murder\" + 0.022*\"face\" + 0.017*\"accus\" + 0.015*\"countri\" + 0.015*\"trial\" + 0.014*\"jail\" + 0.013*\"alleg\"\n",
      "Topic: 6 \n",
      "Words: 0.053*\"polic\" + 0.022*\"melbourn\" + 0.018*\"kill\" + 0.016*\"shoot\" + 0.015*\"arrest\" + 0.014*\"attack\" + 0.012*\"protest\" + 0.012*\"offic\" + 0.011*\"investig\" + 0.010*\"drug\"\n",
      "Topic: 7 \n",
      "Words: 0.035*\"govern\" + 0.017*\"worker\" + 0.016*\"return\" + 0.012*\"group\" + 0.012*\"live\" + 0.009*\"say\" + 0.008*\"work\" + 0.008*\"unit\" + 0.008*\"right\" + 0.008*\"sale\"\n",
      "Topic: 8 \n",
      "Words: 0.024*\"covid\" + 0.024*\"trump\" + 0.020*\"coronavirus\" + 0.020*\"health\" + 0.016*\"rural\" + 0.015*\"victoria\" + 0.014*\"report\" + 0.014*\"news\" + 0.012*\"nation\" + 0.011*\"australia\"\n",
      "Topic: 9 \n",
      "Words: 0.017*\"coast\" + 0.016*\"school\" + 0.015*\"north\" + 0.015*\"rise\" + 0.012*\"flood\" + 0.012*\"price\" + 0.011*\"south\" + 0.011*\"gold\" + 0.011*\"west\" + 0.011*\"farm\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model using **TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1244184"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1244184/1244184 [00:38<00:00, 32521.61it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(tqdm(corpus_tfidf), num_topics=10, id2word=dictionary, passes=2, workers=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.020*\"news\" + 0.016*\"interview\" + 0.014*\"rural\" + 0.012*\"covid\" + 0.011*\"drum\" + 0.010*\"speak\" + 0.009*\"coronavirus\" + 0.008*\"andrew\" + 0.008*\"lockdown\" + 0.007*\"tuesday\"\n",
      "Topic: 1 Word: 0.008*\"morrison\" + 0.008*\"stori\" + 0.008*\"wednesday\" + 0.007*\"david\" + 0.007*\"farmer\" + 0.006*\"coronavirus\" + 0.006*\"drought\" + 0.006*\"quarantin\" + 0.005*\"australia\" + 0.005*\"novemb\"\n",
      "Topic: 2 Word: 0.011*\"govern\" + 0.008*\"health\" + 0.007*\"fund\" + 0.006*\"plan\" + 0.006*\"council\" + 0.005*\"budget\" + 0.005*\"feder\" + 0.005*\"industri\" + 0.005*\"say\" + 0.004*\"sport\"\n",
      "Topic: 3 Word: 0.013*\"polic\" + 0.010*\"crash\" + 0.009*\"death\" + 0.008*\"die\" + 0.008*\"climat\" + 0.008*\"woman\" + 0.007*\"coast\" + 0.007*\"fatal\" + 0.006*\"grandstand\" + 0.006*\"age\"\n",
      "Topic: 4 Word: 0.013*\"donald\" + 0.007*\"thursday\" + 0.007*\"market\" + 0.006*\"australian\" + 0.006*\"share\" + 0.006*\"wall\" + 0.006*\"extend\" + 0.006*\"korea\" + 0.005*\"north\" + 0.005*\"alan\"\n",
      "Topic: 5 Word: 0.022*\"trump\" + 0.006*\"scott\" + 0.006*\"christma\" + 0.005*\"coronavirus\" + 0.005*\"june\" + 0.005*\"cyclon\" + 0.005*\"pandem\" + 0.005*\"search\" + 0.005*\"water\" + 0.005*\"rate\"\n",
      "Topic: 6 Word: 0.016*\"murder\" + 0.014*\"charg\" + 0.013*\"court\" + 0.009*\"sentenc\" + 0.009*\"jail\" + 0.008*\"alleg\" + 0.008*\"accus\" + 0.008*\"child\" + 0.007*\"monday\" + 0.006*\"trial\"\n",
      "Topic: 7 Word: 0.015*\"countri\" + 0.012*\"hour\" + 0.010*\"weather\" + 0.009*\"royal\" + 0.007*\"live\" + 0.007*\"commiss\" + 0.007*\"queensland\" + 0.007*\"coronavirus\" + 0.007*\"victoria\" + 0.006*\"turnbul\"\n",
      "Topic: 8 Word: 0.010*\"kill\" + 0.008*\"violenc\" + 0.006*\"truck\" + 0.006*\"crash\" + 0.006*\"domest\" + 0.005*\"polic\" + 0.005*\"injur\" + 0.004*\"island\" + 0.004*\"ash\" + 0.004*\"dead\"\n",
      "Topic: 9 Word: 0.012*\"australia\" + 0.010*\"world\" + 0.009*\"final\" + 0.008*\"leagu\" + 0.006*\"cricket\" + 0.005*\"australian\" + 0.005*\"rugbi\" + 0.005*\"test\" + 0.005*\"grand\" + 0.005*\"game\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.3972466289997101\t \n",
      "Topic: 0.021*\"elect\" + 0.020*\"chang\" + 0.014*\"hous\" + 0.013*\"busi\" + 0.013*\"council\" + 0.012*\"say\" + 0.012*\"labor\" + 0.011*\"farmer\" + 0.011*\"plan\" + 0.011*\"speak\"\n",
      "\n",
      "Score: 0.3528382480144501\t \n",
      "Topic: 0.035*\"govern\" + 0.017*\"worker\" + 0.016*\"return\" + 0.012*\"group\" + 0.012*\"live\" + 0.009*\"say\" + 0.008*\"work\" + 0.008*\"unit\" + 0.008*\"right\" + 0.008*\"sale\"\n",
      "\n",
      "Score: 0.162348210811615\t \n",
      "Topic: 0.031*\"queensland\" + 0.017*\"world\" + 0.016*\"win\" + 0.013*\"time\" + 0.011*\"royal\" + 0.010*\"beat\" + 0.010*\"australia\" + 0.009*\"commiss\" + 0.008*\"andrew\" + 0.008*\"port\"\n",
      "\n",
      "Score: 0.012511699460446835\t \n",
      "Topic: 0.017*\"coast\" + 0.016*\"school\" + 0.015*\"north\" + 0.015*\"rise\" + 0.012*\"flood\" + 0.012*\"price\" + 0.011*\"south\" + 0.011*\"gold\" + 0.011*\"west\" + 0.011*\"farm\"\n",
      "\n",
      "Score: 0.012509656138718128\t \n",
      "Topic: 0.024*\"covid\" + 0.024*\"trump\" + 0.020*\"coronavirus\" + 0.020*\"health\" + 0.016*\"rural\" + 0.015*\"victoria\" + 0.014*\"report\" + 0.014*\"news\" + 0.012*\"nation\" + 0.011*\"australia\"\n",
      "\n",
      "Score: 0.012509550899267197\t \n",
      "Topic: 0.021*\"interview\" + 0.019*\"open\" + 0.018*\"market\" + 0.017*\"record\" + 0.014*\"final\" + 0.012*\"australian\" + 0.011*\"australia\" + 0.010*\"fall\" + 0.009*\"share\" + 0.009*\"guilti\"\n",
      "\n",
      "Score: 0.012509364634752274\t \n",
      "Topic: 0.031*\"court\" + 0.027*\"charg\" + 0.024*\"case\" + 0.024*\"murder\" + 0.022*\"face\" + 0.017*\"accus\" + 0.015*\"countri\" + 0.015*\"trial\" + 0.014*\"jail\" + 0.013*\"alleg\"\n",
      "\n",
      "Score: 0.012509259395301342\t \n",
      "Topic: 0.018*\"donald\" + 0.015*\"minist\" + 0.014*\"hospit\" + 0.010*\"drive\" + 0.009*\"hobart\" + 0.008*\"hunter\" + 0.008*\"river\" + 0.008*\"northern\" + 0.008*\"award\" + 0.008*\"video\"\n",
      "\n",
      "Score: 0.01250869408249855\t \n",
      "Topic: 0.022*\"crash\" + 0.016*\"die\" + 0.016*\"miss\" + 0.016*\"home\" + 0.014*\"perth\" + 0.013*\"bushfir\" + 0.012*\"death\" + 0.012*\"woman\" + 0.010*\"polic\" + 0.010*\"search\"\n",
      "\n",
      "Score: 0.012508672662079334\t \n",
      "Topic: 0.053*\"polic\" + 0.022*\"melbourn\" + 0.018*\"kill\" + 0.016*\"shoot\" + 0.015*\"arrest\" + 0.014*\"attack\" + 0.012*\"protest\" + 0.012*\"offic\" + 0.011*\"investig\" + 0.010*\"drug\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6909939646720886\t \n",
      "Topic: 0.011*\"govern\" + 0.008*\"health\" + 0.007*\"fund\" + 0.006*\"plan\" + 0.006*\"council\" + 0.005*\"budget\" + 0.005*\"feder\" + 0.005*\"industri\" + 0.005*\"say\" + 0.004*\"sport\"\n",
      "\n",
      "Score: 0.2089741826057434\t \n",
      "Topic: 0.022*\"trump\" + 0.006*\"scott\" + 0.006*\"christma\" + 0.005*\"coronavirus\" + 0.005*\"june\" + 0.005*\"cyclon\" + 0.005*\"pandem\" + 0.005*\"search\" + 0.005*\"water\" + 0.005*\"rate\"\n",
      "\n",
      "Score: 0.012504386715590954\t \n",
      "Topic: 0.013*\"donald\" + 0.007*\"thursday\" + 0.007*\"market\" + 0.006*\"australian\" + 0.006*\"share\" + 0.006*\"wall\" + 0.006*\"extend\" + 0.006*\"korea\" + 0.005*\"north\" + 0.005*\"alan\"\n",
      "\n",
      "Score: 0.012504244223237038\t \n",
      "Topic: 0.015*\"countri\" + 0.012*\"hour\" + 0.010*\"weather\" + 0.009*\"royal\" + 0.007*\"live\" + 0.007*\"commiss\" + 0.007*\"queensland\" + 0.007*\"coronavirus\" + 0.007*\"victoria\" + 0.006*\"turnbul\"\n",
      "\n",
      "Score: 0.012504222802817822\t \n",
      "Topic: 0.008*\"morrison\" + 0.008*\"stori\" + 0.008*\"wednesday\" + 0.007*\"david\" + 0.007*\"farmer\" + 0.006*\"coronavirus\" + 0.006*\"drought\" + 0.006*\"quarantin\" + 0.005*\"australia\" + 0.005*\"novemb\"\n",
      "\n",
      "Score: 0.012504156678915024\t \n",
      "Topic: 0.010*\"kill\" + 0.008*\"violenc\" + 0.006*\"truck\" + 0.006*\"crash\" + 0.006*\"domest\" + 0.005*\"polic\" + 0.005*\"injur\" + 0.004*\"island\" + 0.004*\"ash\" + 0.004*\"dead\"\n",
      "\n",
      "Score: 0.012503908947110176\t \n",
      "Topic: 0.020*\"news\" + 0.016*\"interview\" + 0.014*\"rural\" + 0.012*\"covid\" + 0.011*\"drum\" + 0.010*\"speak\" + 0.009*\"coronavirus\" + 0.008*\"andrew\" + 0.008*\"lockdown\" + 0.007*\"tuesday\"\n",
      "\n",
      "Score: 0.012503726407885551\t \n",
      "Topic: 0.016*\"murder\" + 0.014*\"charg\" + 0.013*\"court\" + 0.009*\"sentenc\" + 0.009*\"jail\" + 0.008*\"alleg\" + 0.008*\"accus\" + 0.008*\"child\" + 0.007*\"monday\" + 0.006*\"trial\"\n",
      "\n",
      "Score: 0.012503656558692455\t \n",
      "Topic: 0.012*\"australia\" + 0.010*\"world\" + 0.009*\"final\" + 0.008*\"leagu\" + 0.006*\"cricket\" + 0.005*\"australian\" + 0.005*\"rugbi\" + 0.005*\"test\" + 0.005*\"grand\" + 0.005*\"game\"\n",
      "\n",
      "Score: 0.012503557838499546\t \n",
      "Topic: 0.013*\"polic\" + 0.010*\"crash\" + 0.009*\"death\" + 0.008*\"die\" + 0.008*\"climat\" + 0.008*\"woman\" + 0.007*\"coast\" + 0.007*\"fatal\" + 0.006*\"grandstand\" + 0.006*\"age\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model using **Coherence Score** and **Perplexity** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  -7.260419823058737\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=processed_docs, dictionary=dictionary, coherence='u_mass', processes=7)\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preplexity :  -9.977694535315248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Preplexity : \",lda_model_tfidf.log_perplexity(corpus_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.2986299395561218\t Topic: 0.010*\"kill\" + 0.008*\"violenc\" + 0.006*\"truck\" + 0.006*\"crash\" + 0.006*\"domest\"\n",
      "Score: 0.26904547214508057\t Topic: 0.016*\"murder\" + 0.014*\"charg\" + 0.013*\"court\" + 0.009*\"sentenc\" + 0.009*\"jail\"\n",
      "Score: 0.21315154433250427\t Topic: 0.013*\"polic\" + 0.010*\"crash\" + 0.009*\"death\" + 0.008*\"die\" + 0.008*\"climat\"\n",
      "Score: 0.0313185378909111\t Topic: 0.011*\"govern\" + 0.008*\"health\" + 0.007*\"fund\" + 0.006*\"plan\" + 0.006*\"council\"\n",
      "Score: 0.03131619840860367\t Topic: 0.008*\"morrison\" + 0.008*\"stori\" + 0.008*\"wednesday\" + 0.007*\"david\" + 0.007*\"farmer\"\n",
      "Score: 0.03131333738565445\t Topic: 0.013*\"donald\" + 0.007*\"thursday\" + 0.007*\"market\" + 0.006*\"australian\" + 0.006*\"share\"\n",
      "Score: 0.03130798041820526\t Topic: 0.012*\"australia\" + 0.010*\"world\" + 0.009*\"final\" + 0.008*\"leagu\" + 0.006*\"cricket\"\n",
      "Score: 0.03130712732672691\t Topic: 0.015*\"countri\" + 0.012*\"hour\" + 0.010*\"weather\" + 0.009*\"royal\" + 0.007*\"live\"\n",
      "Score: 0.031306762248277664\t Topic: 0.022*\"trump\" + 0.006*\"scott\" + 0.006*\"christma\" + 0.005*\"coronavirus\" + 0.005*\"june\"\n",
      "Score: 0.03130309656262398\t Topic: 0.020*\"news\" + 0.016*\"interview\" + 0.014*\"rural\" + 0.012*\"covid\" + 0.011*\"drum\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "tfidf_vector = tfidf[bow_vector]\n",
    "for index, score in sorted(lda_model_tfidf[tfidf_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 540/540 [6:02:34<00:00, 40.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def hyperparameter_tuner():\n",
    "    grid = {}\n",
    "    grid['Validation_Set'] = {}\n",
    "\n",
    "    # Topics range\n",
    "    min_topics = 2\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "    # Alpha parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "\n",
    "    # Beta parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "\n",
    "    # Validation sets\n",
    "    num_of_docs = len(corpus_tfidf)\n",
    "    corpus_sets = [gensim.utils.ClippedCorpus(corpus_tfidf, int(num_of_docs*0.75)), \n",
    "                   corpus_tfidf]\n",
    "\n",
    "    corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "\n",
    "    # Can take a long time to run\n",
    "    if 1 == 1:\n",
    "        pbar = tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "\n",
    "        # iterate through validation corpuses\n",
    "        for i in range(len(corpus_sets)):\n",
    "            # iterate through number of topics\n",
    "            for k in topics_range:\n",
    "                # iterate through alpha values\n",
    "                for a in alpha:\n",
    "                    # iterare through beta values\n",
    "                    for b in beta:\n",
    "                        # get the coherence score for the given parameters\n",
    "                        cv = compute_coherence_values(corpus=corpus_sets[i], \n",
    "                                                      k=k, a=a, b=b)\n",
    "                        # Save the model results\n",
    "                        model_results['Validation_Set'].append(corpus_title[i])\n",
    "                        model_results['Topics'].append(k)\n",
    "                        model_results['Alpha'].append(a)\n",
    "                        model_results['Beta'].append(b)\n",
    "                        model_results['Coherence'].append(cv)\n",
    "\n",
    "                        pbar.update(1)\n",
    "        pd.DataFrame(model_results).to_csv(f'./result-tuner/lda_tuning-result-{datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")}.csv', index=False)\n",
    "        pbar.close()\n",
    "\n",
    "def compute_coherence_values(corpus,k, a, b):\n",
    "\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=k, passes=1, workers=7, alpha=a, eta=b)\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=processed_docs, dictionary=dictionary, coherence='u_mass',processes = 15)\n",
    "\n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "hyperparameter_tuner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Python code for Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Paradox\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook(local=True)\n",
    "from gensim.models import LdaMulticore,CoherenceModel\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class DataGetter:\n",
    "    def __init__(self,dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "    \n",
    "    def get_data(self):\n",
    "        data = pd.read_csv(self.dataset_path, on_bad_lines='skip')\n",
    "        data_text = data[['headline_text']]\n",
    "        data_text['index'] = data_text.index\n",
    "        self.dataset = data_text\n",
    "        return self.dataset\n",
    "    \n",
    "class Preprocessor:\n",
    "    def __init__(self,raw_data,language = 'english'):\n",
    "        self.raw_data = raw_data\n",
    "        self.language = language\n",
    "        self.preprocessed_data = []\n",
    "        \n",
    "        \n",
    "    def stemmer(self,text):\n",
    "        snowball_stemmer = SnowballStemmer(language=self.language)\n",
    "        return snowball_stemmer.stem(text)\n",
    "    \n",
    "    def lemmatizer(self,text):\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        return wordnet_lemmatizer.lemmatize(text, pos='v')\n",
    "\n",
    "    def text_preprocessor(self,text):\n",
    "        preprocessed_text = []\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                preprocessed_text.append(self.stemmer(self.lemmatizer(text)))\n",
    "        return preprocessed_text\n",
    "    \n",
    "    def preprocess(self):\n",
    "        tqdm.pandas()\n",
    "        self.preprocessed_data = self.raw_data['headline_text'].progress_map(self.text_preprocessor)\n",
    "        return self.preprocessed_data\n",
    "\n",
    "class LDAModel:\n",
    "    \n",
    "    def __init__(self,preprocessed_data,vectorization_method = 'tfidf'):\n",
    "        self.preprocessed_data = preprocessed_data\n",
    "        self.vectorization_method = vectorization_method\n",
    "        self.dictionary = gensim.corpora.Dictionary(tqdm(self.preprocessed_data))\n",
    "        self.dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "        self.corpus = []\n",
    "        self.coherence_score = 0\n",
    "    \n",
    "    def fit(self):\n",
    "        if self.vectorization_method == 'bow':\n",
    "            print(\"\\n\\nVectorizing........\\n\")\n",
    "            for i in tqdm(range(len(self.preprocessed_data))):\n",
    "                self.corpus.append(self.dictionary.doc2bow(self.preprocessed_data[i]))\n",
    "            print(\"\\n\\nTraining LDA Model.........\\n\")\n",
    "            self.model = LdaMulticore( self.corpus, num_topics=10, id2word=self.dictionary, passes=2, random_state=100,chunksize=100, workers=7)\n",
    "            \n",
    "        elif self.vectorization_method == 'tfidf':\n",
    "            print(\"\\n\\nVectorizing........\\n\")\n",
    "            for i in tqdm(range(len(self.preprocessed_data))):\n",
    "                self.corpus.append(self.dictionary.doc2bow(self.preprocessed_data[i]))\n",
    "            self.vectorizer = models.TfidfModel(tqdm(self.corpus))\n",
    "            self.corpus = self.vectorizer[self.corpus]\n",
    "            print(\"\\n\\nTraining LDA Model.........\\n\")\n",
    "            self.model = LdaMulticore((self.corpus), num_topics=10, id2word=self.dictionary, passes=2, random_state=100, chunksize=100, workers=7)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def validate_model(self):\n",
    "        # Compute Coherence Score\n",
    "        coherence_model_lda = CoherenceModel(model=self.model, texts=self.preprocessed_data, dictionary=self.dictionary, coherence='u_mass', processes=7)\n",
    "        self.coherence_score = coherence_model_lda.get_coherence()\n",
    "        print('Coherence Score: ', self.coherence_score)\n",
    "        \n",
    "        self.perplexity = self.model.log_perplexity(self.corpus)\n",
    "        print(\"Preplexity : \",self.perplexity)\n",
    "        return {\"coherence score\":self.coherence_score,\"preplexity\":self.perplexity}\n",
    "    \n",
    "    def get_topic_for_new_data(self,new_data):\n",
    "        bow_vector = self.dictionary.doc2bow(preprocess(new_data))\n",
    "        if self.vectorization_method == 'tfidf':\n",
    "            tfidf_vector = self.vectorizer[bow_vector]\n",
    "            for index, score in sorted(self.model[tfidf_vector], key=lambda tup: -1*tup[1]):\n",
    "                print(\"Score: {}\\n Topic: {}\".format(score, self.model.print_topic(index, 5)))\n",
    "        elif self.vectorization_method == 'bow':\n",
    "            for index, score in sorted(self.model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "                print(\"Score: {}\\n Topic: {}\".format(score, self.model.print_topic(index, 5)))\n",
    "            \n",
    "    \n",
    "    def hyperparameter_tuner(self):\n",
    "        grid = {}\n",
    "        grid['Validation_Set'] = {}\n",
    "\n",
    "        # Topics range\n",
    "        min_topics = 2\n",
    "        max_topics = 11\n",
    "        step_size = 1\n",
    "        topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "        # Alpha parameter\n",
    "        alpha = list(np.arange(0.01, 1, 0.3))\n",
    "        alpha.append('symmetric')\n",
    "        alpha.append('asymmetric')\n",
    "\n",
    "        # Beta parameter\n",
    "        beta = list(np.arange(0.01, 1, 0.3))\n",
    "        beta.append('symmetric')\n",
    "        \n",
    "        # Validation sets\n",
    "        num_of_docs = len(self.corpus)\n",
    "        corpus_sets = [gensim.utils.ClippedCorpus(self.corpus, int(num_of_docs*0.75)), \n",
    "                       self.corpus]\n",
    "\n",
    "        corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "        self.model_results = {'Validation_Set': [],\n",
    "                         'Topics': [],\n",
    "                         'Alpha': [],\n",
    "                         'Beta': [],\n",
    "                         'Coherence': []\n",
    "                        }\n",
    "\n",
    "        # Can take a long time to run\n",
    "        if 1 == 1:\n",
    "            pbar = tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "\n",
    "            # iterate through validation corpuses\n",
    "            for i in range(len(corpus_sets)):\n",
    "                # iterate through number of topics\n",
    "                for k in topics_range:\n",
    "                    # iterate through alpha values\n",
    "                    for a in alpha:\n",
    "                        # iterare through beta values\n",
    "                        for b in beta:\n",
    "                            # get the coherence score for the given parameters\n",
    "                            cv = self.compute_coherence_values(corpus=corpus_sets[i], \n",
    "                                                          k=k, a=a, b=b)\n",
    "                            # Save the model results\n",
    "                            self.model_results['Validation_Set'].append(corpus_title[i])\n",
    "                            self.model_results['Topics'].append(k)\n",
    "                            self.model_results['Alpha'].append(a)\n",
    "                            self.model_results['Beta'].append(b)\n",
    "                            self.model_results['Coherence'].append(cv)\n",
    "\n",
    "                            pbar.update(1)\n",
    "            pd.DataFrame(self.model_results).to_csv(f'./result-tuner/lda_tuning-result-{datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")}.csv', index=False)\n",
    "            pbar.close()\n",
    "    \n",
    "    def compute_coherence_values(self, corpus,k, a, b):\n",
    "    \n",
    "        lda_model = LdaMulticore(corpus=corpus, id2word=self.dictionary, num_topics=k, random_state=100, chunksize=100, passes=1, workers=7, alpha=a, eta=b)\n",
    "\n",
    "        coherence_model_lda = CoherenceModel(model=self.model, texts=self.preprocessed_data, dictionary=self.dictionary, coherence='u_mass')\n",
    "\n",
    "        return coherence_model_lda.get_coherence()\n",
    "    \n",
    "    \n",
    "    def get_dictionary(self):\n",
    "        return self.dictionary\n",
    "    \n",
    "    def get_corpus(self):\n",
    "        return self.corpus\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self.vectorizer\n",
    "\n",
    "\n",
    "class LDAVisualizer:\n",
    "    \n",
    "    def __init__(self,model,vectorized_data,vectorizer):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "        self.vectorized_data = vectorized_data\n",
    "    \n",
    "    def visualize_model(self):\n",
    "        self.visualizer = pyLDAvis.gensim_models.prepare(self.model, self.vectorized_data, self.vectorizer)\n",
    "        return self.visualizer\n",
    "    \n",
    "    def save_visualization(self):\n",
    "        pyLDAvis.save_html(self.visualizer, f'./result-vis/lda_result-{datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")}.html')\n",
    "    \n",
    "\n",
    "class Driver:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def run_model(self):\n",
    "        print(\"Loading Dataset.............\\n\")\n",
    "        self.dataset = DataGetter('./abcnews-date-text.csv').get_data()\n",
    "        print(\"Dataset Loaded!\\n\\n\")\n",
    "        print(\"Preprocessing Dataset........\\n\")\n",
    "        self.preprocessed_data = Preprocessor(self.dataset).preprocess()\n",
    "        print(\"Preprocessed Data : \\n\",self.preprocessed_data,end=\"\\n\")\n",
    "        self.model = LDAModel(self.preprocessed_data)\n",
    "        self.model.fit()\n",
    "        print(\"Model :\",self.model.model,end=\"\\n\\n\")\n",
    "#         for idx, topic in self.model.model.print_topics(-1):\n",
    "#             print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "            \n",
    "        self.model.validate_model()\n",
    "            \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset.............\n",
      "\n",
      "Dataset Loaded!\n",
      "\n",
      "\n",
      "Preprocessing Dataset........\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████████████▋                                            | 456661/1244184 [00:44<01:13, 10742.63it/s]"
     ]
    }
   ],
   "source": [
    "code_driver = Driver()\n",
    "lda_model = code_driver.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualizer = LDAVisualizer(lda_model.model,lda_model.get_corpus(),lda_model.get_dictionary())\n",
    "vis = visualizer.visualize_model()\n",
    "visualizer.save_visualization()\n",
    "# pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
